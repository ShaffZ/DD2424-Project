{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import sys\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Dataset and normalization / transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(random_crop=False, horizontal_flip=False,\n",
    "                    translation=False, standardNormalize=False,\n",
    "                      mu=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    Get transformation pipeline for training and test datasets.\n",
    "\n",
    "    Args:\n",
    "        random_crop (int): Padding size to apply random cropping.\n",
    "        horizontal_flip (float): Probability to apply random horizontal flip.\n",
    "        translation (tuple): Maximum absolute fraction for horizontal and vertical translations.\n",
    "        standardNormalize (bool): Whether to apply normalization.\n",
    "        mu (tuple): Mean for normalization.\n",
    "        std (tuple): Standard deviation for normalization.\n",
    "\n",
    "    Returns:\n",
    "        torchvision.transforms.Compose: Transformation pipeline.\n",
    "    \"\"\"\n",
    "    transform_list = []\n",
    "    if random_crop:\n",
    "        transform_list.append(v2.RandomCrop(random_crop, padding=4))\n",
    "    if horizontal_flip:\n",
    "        transform_list.append(v2.RandomHorizontalFlip(horizontal_flip))\n",
    "    if translation:\n",
    "        transform_list.append(v2.RandomAffine(degrees=0, translate=translation))\n",
    "    \n",
    "    transform_list.append(v2.ToTensor())\n",
    "    \n",
    "    if standardNormalize:\n",
    "        transform_list.append(v2.Normalize(mu, std))\n",
    "    \n",
    "    return v2.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu = (0.4914, 0.4822, 0.4465)\n",
    "# std=(0.2023, 0.1994, 0.2010)\n",
    "#random_crop = 32 # crop size\n",
    "#horizontal_flip = 0.5 # random flip probability\n",
    "#translation = (0.1,0.1) # horizontal, vertical\n",
    "batch_size = 64\n",
    "\n",
    "train_transform = get_transforms(standardNormalize=True) # Parameters are defaulted to no change. change parameters as wanted\n",
    "test_transform = get_transforms(standardNormalize=True)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: X=%s, Y=(%s)' % (trainset.data.shape, len(trainset.targets)))\n",
    "print('Test: X=%s, Y=(%s)' % (testset.data.shape, len(testset.targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\text{Output\\_size} = \\left( \\frac{\\text{Input\\_size} - \\text{Kernel\\_size} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right) + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv2d(3, 32, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 32, 32) # (batchsize, channels, height, width)\n",
    "\n",
    "output_tensor = conv_layer(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataLoader, criterion, optimizer, num_epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataLoader):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on the test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "def create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0.0):\n",
    "    return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Block VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model_1block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar10Model_1block,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(32*16*16, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x)) # out = 32,32,32 (channels, height, width)\n",
    "        x = self.relu(self.conv2(x)) # out = 32,32,32\n",
    "        x = self.pool(x) # out = 32,16,16\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x)) # out = 128\n",
    "        x = self.fc2(x) # out = 10\n",
    "        return self.softmax(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model_1block()\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, Loss, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three Block Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar10Model,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3,3), padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3,3), padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=(3,3), padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x)) # out = 32,32,32 (channels, height, width)\n",
    "        x = self.relu(self.conv2(x)) # out = 32,32,32\n",
    "        x = self.pool(x) # out = 32,16,16\n",
    "        \n",
    "        x = self.relu(self.conv3(x)) # out = 64,16,16\n",
    "        x = self.relu(self.conv4(x)) # out = 64,16,16\n",
    "        x = self.pool(x) # out = 64,8,8\n",
    "        \n",
    "        x = self.relu(self.conv5(x)) # out = 128,8,8\n",
    "        x = self.relu(self.conv6(x)) # out = 128,8,8\n",
    "        x = self.pool(x) # out = 128,4,4\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x)) # out = 128\n",
    "        x = self.fc2(x) # out = 10\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inintialising model and it's params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model()\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, Loss, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0, batch_norm=False):\n",
    "        super(Cifar10Model,self).__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if batch_norm else nn.Identity()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if batch_norm else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if batch_norm else nn.Identity()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128) if batch_norm else nn.Identity()\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))  # out = 32,32,32 (channels, height, width)\n",
    "        x = self.relu(self.bn1(self.conv2(x)))  # out = 32,32,32\n",
    "        x = self.pool1(x) # out = 32,16,16\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.conv3(x)))  # out = 64,16,16\n",
    "        x = self.relu(self.bn2(self.conv4(x))) # out = 64,16,16\n",
    "        x = self.pool2(x) # out = 64,8,8\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv5(x))) # out = 128,8,8\n",
    "        x = self.relu(self.bn3(self.conv6(x))) # out = 128,8,8\n",
    "        x = self.pool3(x) # out = 128,4,4\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.bn4(self.fc1(x))) # out = 128\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x) # out = 10\n",
    "        return self.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model(dropout_rate=0.2, batch_norm=False)\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0)\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, Loss, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Weight Decay (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model(dropout_rate=0, batch_norm=False)\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, Loss, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Data Augmentation( horizontal flipping + translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data with augmentation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu = (0.4914, 0.4822, 0.4465)\n",
    "# std=(0.2023, 0.1994, 0.2010)\n",
    "#random_crop = 32 # crop size\n",
    "horizontal_flip = 0.5 # random flip probability\n",
    "translation = (0.1,0.1) # horizontal, vertical\n",
    "batch_size = 64\n",
    "\n",
    "train_transform = get_transforms(standardNormalize=True,horizontal_flip = horizontal_flip, translation = translation) # Parameters are defaulted to no change. change parameters as wanted\n",
    "test_transform = get_transforms(standardNormalize=True)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model(dropout_rate=0, batch_norm=False)\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0.0000)\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, Loss, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
