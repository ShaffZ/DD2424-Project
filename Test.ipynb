{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "\n",
    "N = 10000 # number of samples per file\n",
    "D = 3072 # number of features (dimensions of each picture)\n",
    "K = 10 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadBatch(filename):\n",
    "\t\"\"\" Copied from the dataset website \"\"\"\n",
    "\timport pickle\n",
    "\twith open(filename, 'rb') as fo:\n",
    "\t\tdict = pickle.load(fo, encoding='bytes')\n",
    "\treturn dict\n",
    "\n",
    "def montage(W):\n",
    "\t\"\"\" Display the image for each label in W \"\"\"\n",
    "\tfig, ax = plt.subplots(2,5)\n",
    "\tfor i in range(2):\n",
    "\t\tfor j in range(5):\n",
    "\t\t\tim  = W[i*5+j,:].reshape(32,32,3, order='F')\n",
    "\t\t\tsim = (im-np.min(im[:]))/(np.max(im[:])-np.min(im[:]))\n",
    "\t\t\tsim = sim.transpose(1,0,2)\n",
    "\t\t\tax[i][j].imshow(sim, interpolation='nearest')\n",
    "\t\t\tax[i][j].set_title(\"y=\"+str(5*i+j))\n",
    "\t\t\tax[i][j].axis('off')\n",
    "\tplt.show()\n",
    "\n",
    "def loader(filepath):\n",
    "\n",
    "    dic = LoadBatch(\"Datasets\\\\\" + filepath)\n",
    "    data = dic[b'data'].reshape(-1,3,32,32) # shape = d x n = 3072 x 10000\n",
    "    labels = np.array(dic[b'labels']).flatten() # shape = n = 1000\n",
    "\n",
    "    y = np.zeros((N, K))\n",
    "    for i in range(N):\n",
    "        y[i][labels[i]] = 1\n",
    "\n",
    "    return data, y, labels\n",
    "\n",
    "def extended_loader():\n",
    "    Data = []\n",
    "    OneHot = []\n",
    "    Labels = []\n",
    "    for i in range(5):\n",
    "       D, H, L = loader(f\"data_batch_{i+1}\")\n",
    "       Data.append(D)\n",
    "       OneHot.append(H)\n",
    "       Labels.append(L)\n",
    "    return {\"Data\" :np.vstack(Data), \"OneHot\": np.vstack(np.array(OneHot)),\"Labels\" : np.hstack(Labels)}\n",
    "\n",
    "def val_split(data,val_per=0.02):\n",
    "    n_val = val_per * data[\"Labels\"].shape[0]\n",
    "    split = int(data[\"Labels\"].shape[0] - n_val)\n",
    "    train = {\"Data\" :data[\"Data\"][:, :split], \"OneHot\": data[\"OneHot\"][:, :split],\"Labels\" : data[\"Labels\"][:split]}\n",
    "    val = {\"Data\" :data[\"Data\"][:, split:], \"OneHot\": data[\"OneHot\"][:, split:],\"Labels\" : data[\"Labels\"][split:]}\n",
    "    return train, val\n",
    "\n",
    "def mean_std(train_X):\n",
    "    mean = np.mean(train_X, axis=1, keepdims=True)\n",
    "    std = np.std(train_X, axis=1, keepdims=True)\n",
    "    return mean, std\n",
    "\n",
    "def normalize(mean, std, X):\n",
    "    norm_X = (X - mean) / std\n",
    "    return norm_X\n",
    "\n",
    "def summarize_diagnostics(history):\n",
    " # plot loss\n",
    "    plt.subplot(211)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    # plot accuracy\n",
    "    plt.subplot(212)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "    # save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    plt.savefig(filename + '_plot.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(50000, 3, 32, 32), Y=(50000, 10), y=(50000,)\n",
      "Test: X=(10000, 3, 32, 32), Y=(10000, 10), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "#load and split\n",
    "Data = extended_loader()\n",
    "\n",
    "train_X, train_Y, train_y = Data.values() # Data, one hot labels, labels\n",
    "test_X, test_Y, test_y = loader(\"test_batch\")\n",
    "\n",
    "print('Train: X=%s, Y=%s, y=%s' % (train_X.shape, train_Y.shape, train_y.shape))\n",
    "print('Test: X=%s, Y=%s, y=%s' % (test_X.shape, test_Y.shape, test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m train_Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_Y) \n\u001b[0;32m      4\u001b[0m test_Y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_Y)\n\u001b[1;32m----> 5\u001b[0m train_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m test_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(test_y))\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "train_X = torch.tensor(train_X)\n",
    "test_X = torch.tensor(test_X)\n",
    "train_Y = torch.tensor(train_Y) \n",
    "test_Y = torch.tensor(test_Y)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_y = torch.tensor(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_norm = train_X / 255.0\n",
    "test_X_norm = test_X / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting data in a Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(train_X_norm, train_Y)\n",
    "train_loader = DataLoader(train_set, batch_size= 64, shuffle=True )\n",
    "\n",
    "test_set = TensorDataset(test_X_norm, test_Y)\n",
    "test_loader = DataLoader(test_set, batch_size= test_X_norm.shape[0], shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\text{Output\\_size} = \\left( \\frac{\\text{Input\\_size} - \\text{Kernel\\_size} + 2 \\times \\text{Padding}}{\\text{Stride}} \\right) + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "conv_layer = nn.Conv2d(3, 32, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 32, 32) # (batchsize, channels, height, width)\n",
    "\n",
    "output_tensor = conv_layer(input_tensor)\n",
    "\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Block VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model_1block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar10Model_1block,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(32*16*16, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x)) # out = 32,32,32 (channels, height, width)\n",
    "        x = self.relu(self.conv2(x)) # out = 32,32,32\n",
    "        x = self.pool(x) # out = 32,16,16\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x)) # out = 128\n",
    "        x = self.fc2(x) # out = 10\n",
    "        return self.softmax(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataLoader, Loss, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataLoader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = Loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input, label in testloader:\n",
    "            outputs = model(input)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(label, 1)            \n",
    "            total += label.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on the test set: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model_1block()\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 100] loss: 2.302\n",
      "[Epoch 1, Batch 200] loss: 2.302\n",
      "[Epoch 1, Batch 300] loss: 2.302\n",
      "[Epoch 1, Batch 400] loss: 2.301\n",
      "[Epoch 1, Batch 500] loss: 2.301\n",
      "[Epoch 1, Batch 600] loss: 2.300\n",
      "[Epoch 1, Batch 700] loss: 2.299\n",
      "[Epoch 2, Batch 100] loss: 2.297\n",
      "[Epoch 2, Batch 200] loss: 2.294\n",
      "[Epoch 2, Batch 300] loss: 2.294\n",
      "[Epoch 2, Batch 400] loss: 2.292\n",
      "[Epoch 2, Batch 500] loss: 2.290\n",
      "[Epoch 2, Batch 600] loss: 2.288\n",
      "[Epoch 2, Batch 700] loss: 2.284\n",
      "[Epoch 3, Batch 100] loss: 2.272\n",
      "[Epoch 3, Batch 200] loss: 2.267\n",
      "[Epoch 3, Batch 300] loss: 2.262\n",
      "[Epoch 3, Batch 400] loss: 2.251\n",
      "[Epoch 3, Batch 500] loss: 2.241\n",
      "[Epoch 3, Batch 600] loss: 2.231\n",
      "[Epoch 3, Batch 700] loss: 2.223\n",
      "[Epoch 4, Batch 100] loss: 2.210\n",
      "[Epoch 4, Batch 200] loss: 2.198\n",
      "[Epoch 4, Batch 300] loss: 2.195\n",
      "[Epoch 4, Batch 400] loss: 2.185\n",
      "[Epoch 4, Batch 500] loss: 2.185\n",
      "[Epoch 4, Batch 600] loss: 2.182\n",
      "[Epoch 4, Batch 700] loss: 2.173\n",
      "[Epoch 5, Batch 100] loss: 2.170\n",
      "[Epoch 5, Batch 200] loss: 2.158\n",
      "[Epoch 5, Batch 300] loss: 2.168\n",
      "[Epoch 5, Batch 400] loss: 2.169\n",
      "[Epoch 5, Batch 500] loss: 2.163\n",
      "[Epoch 5, Batch 600] loss: 2.146\n",
      "[Epoch 5, Batch 700] loss: 2.145\n",
      "[Epoch 6, Batch 100] loss: 2.132\n",
      "[Epoch 6, Batch 200] loss: 2.139\n",
      "[Epoch 6, Batch 300] loss: 2.137\n",
      "[Epoch 6, Batch 400] loss: 2.133\n",
      "[Epoch 6, Batch 500] loss: 2.127\n",
      "[Epoch 6, Batch 600] loss: 2.129\n",
      "[Epoch 6, Batch 700] loss: 2.135\n",
      "[Epoch 7, Batch 100] loss: 2.122\n",
      "[Epoch 7, Batch 200] loss: 2.122\n",
      "[Epoch 7, Batch 300] loss: 2.123\n",
      "[Epoch 7, Batch 400] loss: 2.122\n",
      "[Epoch 7, Batch 500] loss: 2.117\n",
      "[Epoch 7, Batch 600] loss: 2.118\n",
      "[Epoch 7, Batch 700] loss: 2.112\n",
      "[Epoch 8, Batch 100] loss: 2.112\n",
      "[Epoch 8, Batch 200] loss: 2.122\n",
      "[Epoch 8, Batch 300] loss: 2.120\n",
      "[Epoch 8, Batch 400] loss: 2.101\n",
      "[Epoch 8, Batch 500] loss: 2.108\n",
      "[Epoch 8, Batch 600] loss: 2.111\n",
      "[Epoch 8, Batch 700] loss: 2.115\n",
      "[Epoch 9, Batch 100] loss: 2.110\n",
      "[Epoch 9, Batch 200] loss: 2.101\n",
      "[Epoch 9, Batch 300] loss: 2.103\n",
      "[Epoch 9, Batch 400] loss: 2.100\n",
      "[Epoch 9, Batch 500] loss: 2.096\n",
      "[Epoch 9, Batch 600] loss: 2.100\n",
      "[Epoch 9, Batch 700] loss: 2.099\n",
      "[Epoch 10, Batch 100] loss: 2.098\n",
      "[Epoch 10, Batch 200] loss: 2.094\n",
      "[Epoch 10, Batch 300] loss: 2.100\n",
      "[Epoch 10, Batch 400] loss: 2.094\n",
      "[Epoch 10, Batch 500] loss: 2.098\n",
      "[Epoch 10, Batch 600] loss: 2.103\n",
      "[Epoch 10, Batch 700] loss: 2.100\n"
     ]
    }
   ],
   "source": [
    "fit(model, train_loader, Loss, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 10.00%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three Block Full Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar10Model,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3,3), padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=(3,3), padding=1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3,3), padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=(3,3), padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x)) # out = 32,32,32 (channels, height, width)\n",
    "        x = self.relu(self.conv2(x)) # out = 32,32,32\n",
    "        x = self.pool(x) # out = 32,16,16\n",
    "        \n",
    "        x = self.relu(self.conv3(x)) # out = 64,16,16\n",
    "        x = self.relu(self.conv4(x)) # out = 64,16,16\n",
    "        x = self.pool(x) # out = 64,8,8\n",
    "        \n",
    "        x = self.relu(self.conv5(x)) # out = 128,8,8\n",
    "        x = self.relu(self.conv6(x)) # out = 128,8,8\n",
    "        x = self.pool(x) # out = 128,4,4\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x)) # out = 128\n",
    "        x = self.fc2(x) # out = 10\n",
    "        return self.softmax(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataLoader, Loss, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(dataLoader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = Loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input, label in testloader:\n",
    "            outputs = model(input)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(label, 1)            \n",
    "            total += label.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on the test set: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inintialising model and it's params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cifar10Model()\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, Loss, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0, batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if batch_norm else nn.Identity()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if batch_norm else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if batch_norm else nn.Identity()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128) if batch_norm else nn.Identity()\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn1(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.conv3(x)))\n",
    "        x = self.relu(self.bn2(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv5(x)))\n",
    "        x = self.relu(self.bn3(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0.0):\n",
    "    return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input, label in testloader:\n",
    "            outputs = model(input)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(label, 1)            \n",
    "            total += label.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "model = Cifar10Model(dropout_rate=0.5, batch_norm=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Weight Decay (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.0, batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if batch_norm else nn.Identity()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64) if batch_norm else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(128) if batch_norm else nn.Identity()\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128) if batch_norm else nn.Identity()\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn1(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.conv3(x)))\n",
    "        x = self.relu(self.bn2(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.conv5(x)))\n",
    "        x = self.relu(self.bn3(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0.0):\n",
    "    return optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "def evaluate_model(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input, label in testloader:\n",
    "            outputs = model(input)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels = torch.max(label, 1)            \n",
    "            total += label.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "model = Cifar10Model(dropout_rate=0, batch_norm=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer(model, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Data Augmentation( horizontal flipping + translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will implement it's simple"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
